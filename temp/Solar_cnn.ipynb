{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13ac045b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    mps_device = torch.device(device)\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = './태양풍/'\n",
    "file_names = [f'ace_{year}.csv' for year in range(1999, 2014)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/785987786.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n"
     ]
    }
   ],
   "source": [
    "# 모든 파일을 읽어와서 데이터프레임 리스트에 저장\n",
    "ace_data_list = [] # 비어있는 데이터 프레임 리스트 만들기\n",
    "for file_name in file_names: # file_name 하나씩 조회\n",
    "    file_path = os.path.join(file_dir, file_name) # file_dir랑 file_name 합쳐서 path 생성\n",
    "    with open(file_path, 'r') as file: # read 모드로 열기\n",
    "        lines = file.readlines() # 라인 읽어오기\n",
    "        data_started = False \n",
    "        data_lines = [] # 비어있는 data_lines 생성\n",
    "        for line in lines: # 읽은 라인들 하나씩 조회\n",
    "            if \"BEGIN DATA\" in line: # 데이터 설명하는 row들이 끝나면\n",
    "                data_started = True # data_started를 true로 바꿈\n",
    "                continue\n",
    "            if data_started: # data_started 이면 \n",
    "                data_lines.append(line) # 라인 읽어서 data_lines에 붙이기\n",
    "        # data_lines 리스트에 저장된 데이터를 데이터프레임으로 변환\n",
    "        ace_data = pd.read_csv(StringIO('\\n'.join(data_lines)), delim_whitespace=True, names=['year', 'doy', 'hr', 'min', 'Np', 'Tp', 'Vp', 'B_gsm_x', 'B_gsm_y', 'B_gsm_z', 'Bt'])\n",
    "        # year, doy, hr, min 열을 하나의 datetime 형식으로 변환\n",
    "        ace_data['datetime'] = pd.to_datetime(ace_data['year'].astype(str) + '-' + ace_data['doy'].astype(str) + ' ' + ace_data['hr'].astype(str) + ':' + ace_data['min'].astype(str), format='%Y-%j %H:%M')\n",
    "        # 데이터프레임의 인덱스를 datetime 열로 설정\n",
    "        ace_data = ace_data.set_index('datetime')\n",
    "        # 특정 값들을 실제 결측값(NaN)으로 대체\n",
    "        ace_data.replace([-9999.900, -9.9999e+03, -9999.90, -9999.9004], np.nan, inplace=True)\n",
    "        # 전처리가 완료된 데이터프레임을 리스트에 추가\n",
    "        ace_data_list.append(ace_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터프레임을 하나로 결합\n",
    "ace_data_combined = pd.concat(ace_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# 결측값이 아닌 데이터의 분포 추정\n",
    "def estimate_distribution(column):\n",
    "    non_na_data = column.dropna()\n",
    "    mu, std = norm.fit(non_na_data)\n",
    "    return mu, std\n",
    "\n",
    "# 결측값을 추정된 분포에서 샘플링한 값으로 대체\n",
    "def fill_missing_with_distribution(df):\n",
    "    filled_df = df.copy() # 받은 데이터 복사본 만들기\n",
    "    for column in df.columns: # column 하나씩 조회\n",
    "        mu, std = estimate_distribution(df[column]) # 평균, 표준편차 구하기\n",
    "        missing_mask = df[column].isna() # 결측치의 위치 표시\n",
    "        filled_values = norm.rvs(loc=mu, scale=std, size=missing_mask.sum()) # 추정된 분포에서 샘플링\n",
    "        filled_df.loc[missing_mask, column] = filled_values # 채워 넣기\n",
    "    return filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 대체\n",
    "ace_data_combined= ace_data_combined.drop(columns=['year', 'doy', 'hr', 'min'])\n",
    "ace_filled = fill_missing_with_distribution(ace_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace_filled.to_csv('./cnn_data/pp_ace.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지자기 교란 지수 데이터 불러오기 및 전처리\n",
    "geomagnetic_data = pd.read_csv('./지자기교란 지수.csv')\n",
    "\n",
    "# 'date' 열을 datetime 형식으로 변환\n",
    "geomagnetic_data['date'] = pd.to_datetime(geomagnetic_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지자기 교란 지수를 3시간 간격으로 전환\n",
    "geomagnetic_long = geomagnetic_data.melt(id_vars=['date'], var_name='hour', value_name='Kp')\n",
    "geomagnetic_long['hour'] = geomagnetic_long['hour'].str.extract('(\\d+)').astype(int)\n",
    "geomagnetic_long['datetime'] = geomagnetic_long.apply(lambda row: row['date'] + pd.Timedelta(hours=row['hour']), axis=1)\n",
    "\n",
    "# datetime을 index로 사용\n",
    "geomagnetic_long = geomagnetic_long.set_index('datetime')\n",
    "# datetime 기준으로 sort\n",
    "geomagnetic_long.sort_values(by='datetime', inplace=True)\n",
    "geomagnetic_long = geomagnetic_long.drop(columns=['date', 'hour'])\n",
    "geomagnetic_long.to_csv('./cnn_data/geomagnetic_long.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectogram 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.stats import norm\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 컬럼별 스펙트로그램 생성 함수\n",
    "def create_spectrogram(data, column_name, sr=60, n_fft=48, hop_length=10, n_mels=24):\n",
    "    data_series = data[column_name].values\n",
    "    S = librosa.feature.melspectrogram(y=data_series, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    # 크기 조정\n",
    "    S_db = librosa.util.fix_length(S_db, size=24, axis=1)\n",
    "    return S_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3시간 단위로 데이터를 나누고 각 구간별로 3D 스펙트로그램 생성 및 파일로 저장\n",
    "def generate_spectrograms_for_intervals(df, interval='3H', output_dir='./3d_spectrograms/'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    interval_data = df.resample(interval)\n",
    "    for start_time, group in interval_data:\n",
    "        spectrograms = []\n",
    "        for column in df.columns:\n",
    "            if not group[column].isnull().all():\n",
    "                spectrogram = create_spectrogram(group, column)\n",
    "                spectrograms.append(spectrogram)\n",
    "        \n",
    "        # 3D 스펙트로그램 생성\n",
    "        if spectrograms:\n",
    "            spectrograms = np.array(spectrograms)\n",
    "            #print(spectrograms.shape[1], spectrograms.shape[2])\n",
    "            # Ensure all spectrograms have the same size\n",
    "            spectrograms_resized = np.array([librosa.util.fix_length(s, size=24, axis=1) for s in spectrograms])\n",
    "            spectrograms_resized = np.array([librosa.util.fix_length(s, size=24, axis=0) for s in spectrograms_resized])\n",
    "            np.save(os.path.join(output_dir, f'spectrogram_{start_time.strftime(\"%Y-%m-%d_%H-%M-%S\")}.npy'), spectrograms_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/257439750.py:5: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  interval_data = df.resample(interval)\n"
     ]
    }
   ],
   "source": [
    "generate_spectrograms_for_intervals(ace_filled, output_dir='./cnn_data/spectogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "def load_data(data_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    indices = []\n",
    "    for i, file in enumerate(os.listdir(data_dir)):\n",
    "        if file.endswith('.npy'):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            spectrogram = np.load(file_path)\n",
    "            X.append(spectrogram)\n",
    "            # 파일 이름에서 datetime 추출\n",
    "            datetime_str = file.split('_')[1] + '_' + file.split('_')[2].split('.')[0]\n",
    "            datetime_obj = pd.to_datetime(datetime_str, format='%Y-%m-%d_%H-%M-%S')\n",
    "            kp_value = geomagnetic_long.loc[datetime_obj]['Kp']\n",
    "            y.append(kp_value)\n",
    "            indices.append(datetime_obj.year)  # 연도를 인덱스에 추가\n",
    "    return np.array(X), np.array(y), np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "X, y, indices = load_data('./cnn_data/spectogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/116763702.py:2: RuntimeWarning: divide by zero encountered in divide\n",
      "  X = X / np.max(X, axis=0)\n",
      "/var/folders/jq/ghsw3hz1139068l6f7s904y40000gn/T/ipykernel_4013/116763702.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  X = X / np.max(X, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# 각 컬럼별로 정규화\n",
    "X = X / np.max(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜별로 학습 데이터와 테스트 데이터로 분할\n",
    "train_indices = np.where(indices < 2013)[0]\n",
    "test_indices = np.where(indices == 2013)[0]\n",
    "\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_test, y_test = X[test_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40912, 7, 24, 24)\n",
      "(40912,)\n",
      "(2920, 7, 24, 24)\n",
      "(2920,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Tensor로 변환\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long) # 정수형으로 변환\n",
    "y_test = torch.tensor(y_test, dtype=torch.long) # 정수형으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40912, 7, 24, 24])\n",
      "torch.Size([2920, 7, 24, 24])\n",
      "torch.Size([40912])\n",
      "torch.Size([2920])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset 및 DataLoader 설정\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 수 계산\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(7, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)  # 3x3은 입력 이미지 크기 24x24이 MaxPooling을 두 번 거친 후의 크기\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 3 * 3)  # Flatten the output\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel(num_classes).to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.43150684931507%\n",
      "RMSE: 1.0549946438869426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(inputs.float())  # ensure inputs are float32\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_labels, all_predictions))\n",
    "print(f'Accuracy: {100 * correct / total}%')\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
